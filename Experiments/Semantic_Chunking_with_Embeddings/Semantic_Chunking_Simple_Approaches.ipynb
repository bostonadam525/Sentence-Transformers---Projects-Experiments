{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "efc2b4dd4c264848a3c22f16ba517f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a05fe18e5a0e4cb995221a77eb1c9a78",
              "IPY_MODEL_5c6be1ae4f2843f9b19ef4c0951fbccf",
              "IPY_MODEL_d427aa55b1664a2ca0806cb4a99e02fb"
            ],
            "layout": "IPY_MODEL_aab3efd275c04715adf651728e3a3c94"
          }
        },
        "a05fe18e5a0e4cb995221a77eb1c9a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a836efb42541c29daa46ae6c4163fc",
            "placeholder": "​",
            "style": "IPY_MODEL_a82f23affe8246a39187ffc26a9a50e4",
            "value": "100%"
          }
        },
        "5c6be1ae4f2843f9b19ef4c0951fbccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74774b883bdd40eebfc8c2de494e1162",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7f447020eb44794a63f1fc89699ada5",
            "value": 6
          }
        },
        "d427aa55b1664a2ca0806cb4a99e02fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e43761b390884eb0b06878842b02a0e7",
            "placeholder": "​",
            "style": "IPY_MODEL_ba4c1ba2160e43738be04c9a254d4c44",
            "value": " 6/6 [00:10&lt;00:00,  1.42s/it]"
          }
        },
        "aab3efd275c04715adf651728e3a3c94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a836efb42541c29daa46ae6c4163fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a82f23affe8246a39187ffc26a9a50e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74774b883bdd40eebfc8c2de494e1162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f447020eb44794a63f1fc89699ada5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e43761b390884eb0b06878842b02a0e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba4c1ba2160e43738be04c9a254d4c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6947f724e47640db80a2281da7db8516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb6e019101254715bea021c4a2c36c60",
              "IPY_MODEL_6b54290b951841518d40327a498c2152",
              "IPY_MODEL_ab1cb4e9a22a4f9e9358f7235bb08ba1"
            ],
            "layout": "IPY_MODEL_26e439fd1f784277bad1cb5cd1fe2246"
          }
        },
        "fb6e019101254715bea021c4a2c36c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b98100c2beb848a68653c643534303f2",
            "placeholder": "​",
            "style": "IPY_MODEL_72856d16e04c49dabfc07664032380e7",
            "value": "100%"
          }
        },
        "6b54290b951841518d40327a498c2152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6968f8d3fc8d496186c620b9dcee8a57",
            "max": 328,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33fd24304ba146bfb6a556b2fe812cc9",
            "value": 328
          }
        },
        "ab1cb4e9a22a4f9e9358f7235bb08ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f7ec0fe54bd4841b521bb3495b3920e",
            "placeholder": "​",
            "style": "IPY_MODEL_a199a2ab21e148e381b863653f8a1c33",
            "value": " 328/328 [00:00&lt;00:00, 687.51it/s]"
          }
        },
        "26e439fd1f784277bad1cb5cd1fe2246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b98100c2beb848a68653c643534303f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72856d16e04c49dabfc07664032380e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6968f8d3fc8d496186c620b9dcee8a57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33fd24304ba146bfb6a556b2fe812cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f7ec0fe54bd4841b521bb3495b3920e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a199a2ab21e148e381b863653f8a1c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8519f05f1ea43aba5e41bd9d4e18fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fea86f1dad7b40fb94daf6219404a877",
              "IPY_MODEL_9d1525821d864c829c91715d1dd79d29",
              "IPY_MODEL_900444a5dad2429c9b934393579adeab"
            ],
            "layout": "IPY_MODEL_eb6298029a5f4159b26fe79905a67497"
          }
        },
        "fea86f1dad7b40fb94daf6219404a877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42effea6bfbe47e8b3aea1b0a801ceb7",
            "placeholder": "​",
            "style": "IPY_MODEL_3827cd61c13241b48e00a76204a9c6ba",
            "value": "100%"
          }
        },
        "9d1525821d864c829c91715d1dd79d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abaf97de0ca140988ee62a5a8a8b5dbc",
            "max": 329,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e71a8f2139dc40a281e29605eb4885b9",
            "value": 329
          }
        },
        "900444a5dad2429c9b934393579adeab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_129d612659394f019211ed3c95cecec0",
            "placeholder": "​",
            "style": "IPY_MODEL_75b327dccc4d44d1aef5482402799479",
            "value": " 329/329 [00:18&lt;00:00, 16.09it/s]"
          }
        },
        "eb6298029a5f4159b26fe79905a67497": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42effea6bfbe47e8b3aea1b0a801ceb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3827cd61c13241b48e00a76204a9c6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abaf97de0ca140988ee62a5a8a8b5dbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e71a8f2139dc40a281e29605eb4885b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "129d612659394f019211ed3c95cecec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75b327dccc4d44d1aef5482402799479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Chunking - Simple Approaches\n",
        "* Notebook by Adam Lang\n",
        "* Date: 9/16/2024\n",
        "\n",
        "# Overview\n",
        "* In this notebook we will experiment with various semantic chunkers that are often used with LLMs and RAG and in NLP in general.\n",
        "* The semantic chunkers we are going to experiment with are found here: https://github.com/aurelio-labs/semantic-chunkers"
      ],
      "metadata": {
        "id": "u8WrgR1M9ZZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "Qv5VTATu9-_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NjQx3M4M9SUk"
      },
      "outputs": [],
      "source": [
        "## install\n",
        "!pip install -qU \\\n",
        "  semantic-chunkers==0.0.3 \\\n",
        "  datasets==2.19.1 ## huggingface datasets just for experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Chunkers\n",
        "* These can be used on multi-modal data (e.g. audio, video, text, RAG, splitting, etc..).\n",
        "* The examples here are more focused on RAG (Retrieval Augmented Generation).\n",
        "* There are 3 main types of semantic chunkers to try here:\n",
        "1. `StatisticalChunker`\n",
        "   *  Statistical chunking method is the most robust chunking method,\n",
        "   * Uses a **varying similarity threshold** to identify more dynamic and local similarity splits.\n",
        "   * Gives a good balance between accuracy and efficiency\n",
        "   * However, can ONLY be used for text documents (unlike the multi-modal ConsecutiveChunker).\n",
        "   * Pros of this chunker:\n",
        "    * can automatically identify a custom threshold value to use while chunking text.\n",
        "    * requires less customization than other chunkers.\n",
        "2. `ConsecutiveChunker`\n",
        "   * Simplest method of chunking.\n",
        "3. `CumulativeChunker`\n",
        "  * More compute intensive process.\n",
        "  * Can often provide more stable results as it is more noise resistant.\n",
        "  * Very expensive in both time and money.\n",
        "\n"
      ],
      "metadata": {
        "id": "82Vkcl02-Vrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Datasets for testing\n",
        "* We will use huggingface datasets."
      ],
      "metadata": {
        "id": "feySpu8o_uO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "## data\n",
        "data = load_dataset('jamescalam/ai-arxiv2', split='train')\n",
        "# view data\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdPVZ8Qn-IbN",
        "outputId": "150708b2-c53f-4f32-d729-3513c54806c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'content', 'references'],\n",
              "    num_rows: 2673\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## view 1 of the arxiv papers from dataset\n",
        "content = data[3][\"content\"]\n",
        "print(content[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ifH5CII_7O3",
        "outputId": "041bf5aa-084e-43bc-8011-6f00df22c3bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
            "# Albert Gu*1 and Tri Dao*2\n",
            "1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Princeton University agu@cs.cmu.edu, tri@tridao.me\n",
            "# Abstract\n",
            "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ computational ineï¬ciency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## split the dataset\n",
        "content = content[:20_000]"
      ],
      "metadata": {
        "id": "bs1ocK2LAIOd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoders\n",
        "* Every chunker requires an `encoder`.\n",
        "* We can use open source encoders such as `HuggingfaceEncoder` or `FastembedEncoder`.\n",
        "* We can also use closed source proprietry encoders such as `OpenAIEncoder` or `CohereEncoder`."
      ],
      "metadata": {
        "id": "SOCm_Y7RAb10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## if you were using openai\n",
        "## import os\n",
        "## from getpass import getpass\n",
        "## from semantic_router.encoders import OpenAIEncoder"
      ],
      "metadata": {
        "id": "FB6m2yYEAXDG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## install sentence transformers\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTn29upODCQV",
        "outputId": "9cc30884-3cb6-47b9-f4b9-ff70adf91dfc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Encoder\n",
        "* The default embedding model is: `sentence-transformers/all-MiniLM-L6-v2`.\n",
        "\n"
      ],
      "metadata": {
        "id": "mv41l4IKD8X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## using a HuggingfaceEncoder\n",
        "from semantic_router.encoders import HuggingFaceEncoder\n",
        "\n",
        "\n",
        "## instantiate encoder model\n",
        "encoder = HuggingFaceEncoder() ## use any open source model you wish"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5zpKxxJA3jc",
        "outputId": "f404a90d-5e2b-4b87-9e4c-2f18166e938f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Statistical Chunking\n",
        "* This is the best OOTB solution as it will determine the parameters of your chunking for you.\n",
        "* Cost effective\n",
        "* FAST"
      ],
      "metadata": {
        "id": "vRjh7wMTEPlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## statistical chunker\n",
        "from semantic_chunkers import StatisticalChunker\n",
        "\n",
        "##setup chunker\n",
        "stat_chunker = StatisticalChunker(encoder=encoder)"
      ],
      "metadata": {
        "id": "Hz-FfV8eB0B3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create statistical chunks\n",
        "stat_chunks = stat_chunker(docs=[content])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx3Up1xbE6Xm",
        "outputId": "b0bd4439-09e8-44d7-ab71-d8a4173e4fe1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-09-16 18:25:43 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 300. Splitting to sentences before semantically merging.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## test out chunks\n",
        "stat_chunker.print(stat_chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJQu7FCzFLfA",
        "outputId": "1524c53b-89a9-4e66-cdd9-2669977d8dd5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1, tokens 300, triggered by: token limit\n",
            "\u001b[31m# Mamba: Linear-Time Sequence Modeling with Selective State Spaces # Albert Gu*1 and Tri Dao*2 1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Princeton University agu@cs.cmu.edu, tri@tridao.me # Abstract Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ computational ineï¬ ciency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of eï¬ cient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpliï¬ ed end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5Ã higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 2, tokens 300, triggered by: token limit\n",
            "\u001b[32mAs a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. # 1 Introduction Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an eï¬ ective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The eï¬ cacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 3, tokens 112, triggered by: 0.10\n",
            "\u001b[34mHowever, this property brings fundamental drawbacks: an inability to model anything outside of a ï¬ nite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more eï¬ cient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it eï¬ ective. As of yet, none of these variants have been shown to be empirically eï¬ ective at scale across domains.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 4, tokens 127, triggered by: 0.06\n",
            "\u001b[35mRecently, structured state space sequence models (SSMs) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very eï¬ ciently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 5, tokens 216, triggered by: 0.13\n",
            "\u001b[31mEqual contribution. 1 mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al. 2021). Many ï¬ avors of SSMs (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less eï¬ ective at modeling discrete and information-dense data such as text.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 6, tokens 129, triggered by: 0.09\n",
            "\u001b[32mWe propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to eï¬ ciently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to ï¬ lter out irrelevant information and remember relevant information indeï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 7, tokens 207, triggered by: token limit\n",
            "\u001b[34mnitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally eï¬ cient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between diï¬ erent levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3Ã faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 8, tokens 174, triggered by: 0.15\n",
            "\u001b[35mSelective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and eï¬ ciency together yield performance improvements on real data up to sequence length 1M. We empirically validate Mambaâ s potential as a general sequence FM backbone, in both pretraining quality and domain-speciï¬ c task performance, on several types of modalities and settings:\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 9, tokens 283, triggered by: token limit\n",
            "\u001b[31mâ ¢ Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indeï¬ nitely long (>1M tokens). â ¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform- ers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. â ¢ Language Modeling. Mamba is the ï¬ rst linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5Ã generation throughput compared to Transformers of similar size, and Mamba-3Bâ s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 10, tokens 159, triggered by: 0.18\n",
            "\u001b[32mModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. 2 # Selective State Space Model # with Hardware-aware State Expansion # A vuvy GPU SRAM Selection Mechanism es Selection Mechanism Figure 1: (Overview.) Structured SSMs independently map each channel (e.g. ð · = 5) of an input ð ¥ to output ð ¦ through a higher dimensional latent state â (e.g. ð = 4). Prior SSMs avoid materializing this large effective state (ð ·ð , times batch size ð µ and sequence length ð ¿) through clever alternate computation paths requiring time-invariance: the (â , A, B, C) parameters are constant across time.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 11, tokens 100, triggered by: -0.00\n",
            "\u001b[34mOur selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. # 2 State Space Models Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional function or sequence ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 12, tokens 206, triggered by: 0.20\n",
            "\u001b[35m¥(ð ¡) â â â ¦ ð ¦(ð ¡) â â through an implicit latent state â (ð ¡) â â ð . Concretely, S4 models are deï¬ ned with four parameters (â , A, B, C), which deï¬ ne a sequence-to-sequence trans- formation in two stages. â â ²(ð ¡) = Aâ (ð ¡) + Bð ¥(ð ¡) ð ¦(ð ¡) = Câ (ð ¡) (1a) (1b) â ð ¡ = Aâ ð ¡â 1 + Bð ¥ð ¡ ð ¦ð ¡ = Câ ð ¡ (2a) (2b) ð ð ² = (Cð ©, Cð ¨ð ©, â ¦ , Cð ¨ ð ¦ = ð ¥ â ð ² ð ©, â ¦ ) (3a) (3b) Discretization. The ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 13, tokens 129, triggered by: -0.04\n",
            "\u001b[31mrst stage transforms the â continuous parametersâ (â , A, B) to â discrete parametersâ (A, B) through ï¬ xed formulas A = ð ð ´(â , A) and B = ð ð µ(â , A, B), where the pair (ð ð ´, ð ð µ) is called a discretization rule. Various rules can be used such as the zero-order hold (ZOH) deï¬ ned in equation (4). A = exp(â A) B = (â A)â 1(exp(â A) â I) â â B (4)\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 14, tokens 157, triggered by: 0.09\n",
            "\u001b[32mDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized (Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023). It also has connections to gating mechanisms of RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5. However, from a mechanical point of view discretization can simply be viewed as the ï¬ rst step of the computation graph in the forward pass of an SSM.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 15, tokens 217, triggered by: 0.18\n",
            "\u001b[34mAlternate ï¬ avors of SSMs can bypass the discretization step and parameterize (A, B) directly instead (Zhang et al. 2023), which may be easier to reason about. Computation. After the parameters have been transformed from (â , A, B, C) â ¦ (A, B, C), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3). 3 Commonly, the model uses the convolutional mode (3) for eï¬ cient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for eï¬ cient autoregressive inference (where the inputs are seen one timestep at a time). Linear Time Invariance (LTI). An important property of equations (1) to (3) is that the modelâ s dynamics are constant through time. In other words (â , A, B, C), and consequently (A, B) as well, are ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 16, tokens 156, triggered by: 0.20\n",
            "\u001b[35mxed for all time-steps. This property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions. Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models. Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental eï¬ ciency constraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the eï¬ ciency bottlenecks.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 17, tokens 141, triggered by: 0.10\n",
            "\u001b[31mStructure and Dimensions. Finally, we note that structured SSMs are so named because computing them eï¬ ciently also requires imposing structure on the A matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use. In this case, the A â â ð Ã ð , B â â ð Ã 1, C â â 1Ã ð matrices can all be represented by ð numbers. To operate over an input sequence ð ¥ of batch size ð µ and length ð ¿ with ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 18, tokens 236, triggered by: 0.15\n",
            "\u001b[32m· channels, the SSM is applied independently to each channel. Note that in this case, the total hidden state has dimension ð ·ð per input, and computing it over the sequence length requires ð (ð µð ¿ð ·ð ) time and memory; this is the root of the fundamental eï¬ ciency bottleneck addressed in Section 3.3. General State Space Models. We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in diï¬ erent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman ï¬ lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning). Throughout this entire paper we use the term â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 19, tokens 229, triggered by: 0.15\n",
            "\u001b[34mSSMâ to refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 20, tokens 249, triggered by: 0.21\n",
            "\u001b[35mâ ¢ Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. â ¢ H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. â ¢ Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). â ¢ RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. 4 â ¢ RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation (attention-free Transformer (S.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 21, tokens 187, triggered by: 0.08\n",
            "\u001b[31mZhai et al. 2021)). Its main â WKVâ mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. # 3 Selective State Space Models We motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 22, tokens 143, triggered by: 0.21\n",
            "\u001b[32mciently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). # 3.1 Motivation: Selection as a Means of Compression We argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoï¬ s of popular sequence models from this point of view. For example, attention is both eï¬ ective and ineï¬ cient because it explicitly does not compress context at all.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 23, tokens 111, triggered by: 0.22\n",
            "\u001b[34mThis can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are eï¬ cient because they have a ï¬ nite state, implying constant-time inference and linear-time training. However, their eï¬ ectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 24, tokens 283, triggered by: 0.12\n",
            "\u001b[35mâ ¢ The Selective Copying task modiï¬ es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and ï¬ lter out the irrelevant ones (white). â ¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the (A, B) transitions in (2)) cannot let them select the correct information from their context, or aï¬ ect the hidden state passed along the sequence an in input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have diï¬ culty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 25, tokens 174, triggered by: final split\n",
            "\u001b[31mIn summary, the eï¬ ciency vs. eï¬ ectiveness tradeoï¬ of sequence models is characterized by how well they compress their state: eï¬ cient models must have a small state, while eï¬ ective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or ï¬ lter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). # Improving SSMs with Selection One method of incorporating a selection mechanism into models is by letting their parameters that aï¬ ect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the c\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## stat_chunks\n",
        "stat_chunks[0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uufqVyNDFeWd",
        "outputId": "2280bada-3a06-4b5a-852c-e73ad7d28b33"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Chunk(splits=['# Mamba:', 'Linear-Time Sequence Modeling with Selective State Spaces', '# Albert Gu*1 and Tri Dao*2', '1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Princeton University agu@cs.cmu.edu, tri@tridao.me', '# Abstract', 'Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module.', 'Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ', 'computational ineï¬', 'ciency on long sequences, but they have not performed as well as attention on important modalities such as language.', 'We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements.', 'First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.', 'Second, even though this change prevents the use of eï¬', 'cient convolutions, we design a hardware-aware parallel algorithm in recurrent mode.', 'We integrate these selective SSMs into a simpliï¬', 'ed end-to-end neural network architecture without attention or even MLP blocks (Mamba).', 'Mamba enjoys fast inference (5Ã', 'higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences.'], is_triggered=False, triggered_score=None, token_count=300, metadata=None),\n",
              "  Chunk(splits=['As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics.', 'On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.', '# 1 Introduction', 'Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an eï¬', 'ective paradigm in modern machine learning.', 'The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014).', 'While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The eï¬', 'cacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data.'], is_triggered=False, triggered_score=None, token_count=300, metadata=None),\n",
              "  Chunk(splits=['However, this property brings fundamental drawbacks: an inability to model anything outside of a ï¬', 'nite window, and quadratic scaling with respect to the window length.', 'An enormous body of research has appeared on more eï¬', 'cient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it eï¬', 'ective.', 'As of yet, none of these variants have been shown to be empirically eï¬', 'ective at scale across domains.'], is_triggered=True, triggered_score=0.09920267290709298, token_count=112, metadata=None),\n",
              "  Chunk(splits=['Recently, structured state space sequence models (SSMs) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling.', 'These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models (Kalman 1960).', 'This class of models can be computed very eï¬', 'ciently as either a recurrence or convolution, with linear or near-linear scaling in sequence length.', 'Additionally, they have principled'], is_triggered=True, triggered_score=0.06348165084543538, token_count=127, metadata=None),\n",
              "  Chunk(splits=['Equal contribution.', '1', 'mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al. 2021).', 'Many ï¬', 'avors of SSMs (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y.', 'Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023).', 'However, they have been less eï¬', 'ective at modeling discrete and information-dense data such as text.'], is_triggered=True, triggered_score=0.12519296118931986, token_count=216, metadata=None),\n",
              "  Chunk(splits=['We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.', 'Selection Mechanism.', 'First, we identify a key limitation of prior models: the ability to eï¬', 'ciently select data in an input-dependent manner (i.e. focus on or ignore particular inputs).', 'Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input.', 'This allows the model to ï¬', 'lter out irrelevant information and remember relevant information indeï¬'], is_triggered=True, triggered_score=0.09413109656650988, token_count=129, metadata=None),\n",
              "  Chunk(splits=['nitely.', 'Hardware-aware Algorithm.', 'This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally eï¬', 'cient.', 'We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between diï¬', 'erent levels of the GPU memory hierarchy.', 'The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3Ã', 'faster on A100 GPUs).', 'Architecture.', 'We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces.'], is_triggered=False, triggered_score=None, token_count=207, metadata=None),\n",
              "  Chunk(splits=['Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and eï¬', 'ciency together yield performance improvements on real data up to sequence length 1M.', 'We empirically validate Mambaâ', 's potential as a general sequence FM backbone, in both pretraining quality and domain-speciï¬', 'c task performance, on several types of modalities and settings:'], is_triggered=True, triggered_score=0.14953974520211802, token_count=174, metadata=None),\n",
              "  Chunk(splits=['â', '¢ Synthetics.', 'On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indeï¬', 'nitely long (>1M tokens).', 'â', '¢ Audio and Genomics.', 'Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform- ers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half).', 'In both settings, its performance improves with longer context up to million-length sequences.', 'â', '¢ Language Modeling.', 'Mamba is the ï¬', 'rst linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations.', 'With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023).', 'Our Mamba language model has 5Ã', 'generation throughput compared to Transformers of similar size, and Mamba-3Bâ', 's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).'], is_triggered=False, triggered_score=None, token_count=283, metadata=None),\n",
              "  Chunk(splits=['Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.', '2', '# Selective State Space Model', '# with Hardware-aware State Expansion', '# A', 'vuvy GPU SRAM Selection Mechanism es', 'Selection Mechanism', 'Figure 1: (Overview.) Structured SSMs independently map each channel (e.g. ð', '· = 5) of an input ð', '¥ to output ð', '¦ through a higher dimensional latent state â', '(e.g. ð', '= 4).', 'Prior SSMs avoid materializing this large effective state (ð', '·ð', ', times batch size ð', 'µ and sequence length ð', '¿) through clever alternate computation paths requiring time-invariance: the (â', ', A, B, C) parameters are constant across time.'], is_triggered=True, triggered_score=0.18127568711455397, token_count=159, metadata=None),\n",
              "  Chunk(splits=['Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy.', '# 2 State Space Models', 'Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models.', 'They are inspired by a particular continuous system (1) that maps a 1-dimensional function or sequence ð'], is_triggered=True, triggered_score=-0.000480355780437635, token_count=100, metadata=None),\n",
              "  Chunk(splits=['¥(ð', '¡) â', 'â', 'â', '¦ ð', '¦(ð', '¡) â', 'â', 'through an implicit latent state â', '(ð', '¡) â', 'â', 'ð', '.', 'Concretely, S4 models are deï¬', 'ned with four parameters (â', ', A, B, C), which deï¬', 'ne a sequence-to-sequence trans- formation in two stages.', 'â', 'â', '²(ð', '¡) = Aâ', '(ð', '¡) + Bð', '¥(ð', '¡) ð', '¦(ð', '¡) = Câ', '(ð', '¡)', '(1a) (1b) â', 'ð', '¡ = Aâ', 'ð', '¡â', '1 + Bð', '¥ð', '¡ ð', '¦ð', '¡ = Câ', 'ð', '¡ (2a) (2b) ð', 'ð', '² = (Cð', '©, Cð', '¨ð', '©, â', '¦ , Cð', '¨ ð', '¦ = ð', '¥ â', 'ð', '² ð', '©, â', '¦ ) (3a) (3b)', 'Discretization.', 'The ï¬'], is_triggered=True, triggered_score=0.203779624364777, token_count=206, metadata=None),\n",
              "  Chunk(splits=['rst stage transforms the â', 'continuous parametersâ', '(â', ', A, B) to â', 'discrete parametersâ', '(A, B) through ï¬', 'xed formulas A = ð', 'ð', '´(â', ', A) and B = ð', 'ð', 'µ(â', ', A, B), where the pair (ð', 'ð', '´, ð', 'ð', 'µ) is called a discretization rule.', 'Various rules can be used such as the zero-order hold (ZOH) deï¬', 'ned in equation (4).', 'A = exp(â', 'A) B = (â', 'A)â', '1(exp(â', 'A) â', 'I) â', 'â', 'B (4)'], is_triggered=True, triggered_score=-0.03698648245945706, token_count=129, metadata=None),\n",
              "  Chunk(splits=['Discretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized (Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023).', 'It also has connections to gating mechanisms of RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5.', 'However, from a mechanical point of view discretization can simply be viewed as the ï¬', 'rst step of the computation graph in the forward pass of an SSM.'], is_triggered=True, triggered_score=0.0944589863042761, token_count=157, metadata=None),\n",
              "  Chunk(splits=['Alternate ï¬', 'avors of SSMs can bypass the discretization step and parameterize (A, B) directly instead (Zhang et al. 2023), which may be easier to reason about.', 'Computation.', 'After the parameters have been transformed from (â', ', A, B, C) â', '¦ (A, B, C), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3).', '3', 'Commonly, the model uses the convolutional mode (3) for eï¬', 'cient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for eï¬', 'cient autoregressive inference (where the inputs are seen one timestep at a time).', 'Linear Time Invariance (LTI).', 'An important property of equations (1) to (3) is that the modelâ', 's dynamics are constant through time.', 'In other words (â', ', A, B, C), and consequently (A, B) as well, are ï¬'], is_triggered=True, triggered_score=0.1799997108985984, token_count=217, metadata=None),\n",
              "  Chunk(splits=['xed for all time-steps.', 'This property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions.', 'Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models.', 'Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental eï¬', 'ciency constraints, discussed in Section 3.3.', 'However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the eï¬', 'ciency bottlenecks.'], is_triggered=True, triggered_score=0.19914144157678193, token_count=156, metadata=None),\n",
              "  Chunk(splits=['Structure and Dimensions.', 'Finally, we note that structured SSMs are so named because computing them eï¬', 'ciently also requires imposing structure on the A matrix.', 'The most popular form of structure is diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.', 'In this case, the A â', 'â', 'ð', 'Ã', 'ð', ', B â', 'â', 'ð', 'Ã', '1, C â', 'â', '1Ã', 'ð', 'matrices can all be represented by ð', 'numbers.', 'To operate over an input sequence ð', '¥ of batch size ð', 'µ and length ð', '¿ with ð'], is_triggered=True, triggered_score=0.10406992360559941, token_count=141, metadata=None),\n",
              "  Chunk(splits=['· channels, the SSM is applied independently to each channel.', 'Note that in this case, the total hidden state has dimension ð', '·ð', 'per input, and computing it over the sequence length requires ð', '(ð', 'µð', '¿ð', '·ð', ') time and memory; this is the root of the fundamental eï¬', 'ciency bottleneck addressed in Section 3.3.', 'General State Space Models.', 'We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state.', 'It has been used to refer to many disparate concepts in diï¬', 'erent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman ï¬', 'lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).', 'Throughout this entire paper we use the term â'], is_triggered=True, triggered_score=0.1481264956874038, token_count=236, metadata=None),\n",
              "  Chunk(splits=['SSMâ', 'to refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably.', 'For convenience we may also include derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y.', 'Li et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary.', 'SSM Architectures.', 'SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines.'], is_triggered=True, triggered_score=0.14526849890577678, token_count=229, metadata=None),\n",
              "  Chunk(splits=['â', '¢ Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM.', 'â', '¢ H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3).', 'H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer.', 'â', '¢ Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021).', 'â', '¢ RetNet (Y.', 'Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.', '4', 'â', '¢ RWKV (B.', 'Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation (attention-free Transformer (S.'], is_triggered=True, triggered_score=0.20886992677162192, token_count=249, metadata=None),\n",
              "  Chunk(splits=['Zhai et al. 2021)).', 'Its main â', 'WKVâ', 'mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs.', 'Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B).', 'We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM.', '# 3 Selective State Space Models', 'We motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2).', 'The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them eï¬'], is_triggered=True, triggered_score=0.07761379059587353, token_count=187, metadata=None),\n",
              "  Chunk(splits=['ciently.', 'We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3).', 'We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4).', 'Finally, we discuss some additional properties of selection mechanisms (Section 3.5).', '# 3.1 Motivation:', 'Selection as a Means of Compression', 'We argue that a fundamental problem of sequence modeling is compressing context into a smaller state.', 'In fact, we can view the tradeoï¬', 's of popular sequence models from this point of view.', 'For example, attention is both eï¬', 'ective and ineï¬', 'cient because it explicitly does not compress context at all.'], is_triggered=True, triggered_score=0.21205174697722146, token_count=143, metadata=None),\n",
              "  Chunk(splits=['This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers.', 'On the other hand, recurrent models are eï¬', 'cient because they have a ï¬', 'nite state, implying constant-time inference and linear-time training.', 'However, their eï¬', 'ectiveness is limited by how well this state has compressed the context.', 'To understand this principle, we focus on two running examples of synthetic tasks (Figure 2).'], is_triggered=True, triggered_score=0.22014869283611765, token_count=111, metadata=None),\n",
              "  Chunk(splits=['â', '¢ The Selective Copying task modiï¬', 'es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize.', 'It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and ï¬', 'lter out the irrelevant ones (white).', 'â', '¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022).', 'It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black).', 'These tasks reveal the failure mode of LTI models.', 'From the recurrent view, their constant dynamics (e.g. the (A, B) transitions in (2)) cannot let them select the correct information from their context, or aï¬', 'ect the hidden state passed along the sequence an in input-dependent way.', 'From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have diï¬', 'culty with the Selective Copying task because of lack of content-awareness (Figure 2).', 'More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels.'], is_triggered=True, triggered_score=0.12241282345528769, token_count=283, metadata=None),\n",
              "  Chunk(splits=['In summary, the eï¬', 'ciency vs. eï¬', 'ectiveness tradeoï¬', 'of sequence models is characterized by how well they compress their state: eï¬', 'cient models must have a small state, while eï¬', 'ective models must have a state that contains all necessary information from the context.', 'In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or ï¬', 'lter out inputs into a sequential state.', 'In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion).', '# Improving SSMs with Selection', 'One method of incorporating a selection mechanism into models is by letting their parameters that aï¬', 'ect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the c'], is_triggered=False, triggered_score=None, token_count=174, metadata=None)]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Consecutive Chunking\n",
        "* Simplest version of semantic chunking.\n",
        "* Most encoders require various scoring thresholds.\n",
        "* As an example, OpenAI ada-text-embedding uses similarity threshold of 0.7 to 0.8.\n",
        "* Newer text embedding models such as ada-text-embedding-small uses similarity threshold of 0.3 (smaller thresholds).\n",
        "\n",
        "### How Consecutive Chunking Works\n",
        "* Looks for drop in similarity score and defines a chunk."
      ],
      "metadata": {
        "id": "ng3D57XjGQCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from semantic_chunkers import ConsecutiveChunker\n",
        "\n",
        "## setup chunker\n",
        "cons_chunker = ConsecutiveChunker(encoder=encoder, score_threshold=0.3)"
      ],
      "metadata": {
        "id": "_3-HAn6uFvjK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create consecutive chunks\n",
        "cons_chunks = cons_chunker(docs=[content])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "efc2b4dd4c264848a3c22f16ba517f02",
            "a05fe18e5a0e4cb995221a77eb1c9a78",
            "5c6be1ae4f2843f9b19ef4c0951fbccf",
            "d427aa55b1664a2ca0806cb4a99e02fb",
            "aab3efd275c04715adf651728e3a3c94",
            "68a836efb42541c29daa46ae6c4163fc",
            "a82f23affe8246a39187ffc26a9a50e4",
            "74774b883bdd40eebfc8c2de494e1162",
            "d7f447020eb44794a63f1fc89699ada5",
            "e43761b390884eb0b06878842b02a0e7",
            "ba4c1ba2160e43738be04c9a254d4c44",
            "6947f724e47640db80a2281da7db8516",
            "fb6e019101254715bea021c4a2c36c60",
            "6b54290b951841518d40327a498c2152",
            "ab1cb4e9a22a4f9e9358f7235bb08ba1",
            "26e439fd1f784277bad1cb5cd1fe2246",
            "b98100c2beb848a68653c643534303f2",
            "72856d16e04c49dabfc07664032380e7",
            "6968f8d3fc8d496186c620b9dcee8a57",
            "33fd24304ba146bfb6a556b2fe812cc9",
            "0f7ec0fe54bd4841b521bb3495b3920e",
            "a199a2ab21e148e381b863653f8a1c33"
          ]
        },
        "id": "pHdN1KwsKTH1",
        "outputId": "6706a42c-9267-42b8-d202-e2b5d34eade4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efc2b4dd4c264848a3c22f16ba517f02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/328 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6947f724e47640db80a2281da7db8516"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## print chunks\n",
        "cons_chunker.print(cons_chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-vHlQ0bKYfZ",
        "outputId": "ba1daf4b-f959-4108-9b51-41d0fe1cc980"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1, tokens None, triggered by: 0.06\n",
            "\u001b[31m# Mamba:\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 2, tokens None, triggered by: 0.08\n",
            "\u001b[32mLinear-Time Sequence Modeling with Selective State Spaces\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 3, tokens None, triggered by: 0.08\n",
            "\u001b[34m# Albert Gu*1 and Tri Dao*2\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 4, tokens None, triggered by: 0.05\n",
            "\u001b[35m1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Princeton University agu@cs.cmu.edu, tri@tridao.me\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 5, tokens None, triggered by: 0.15\n",
            "\u001b[31m# Abstract\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 6, tokens None, triggered by: 0.12\n",
            "\u001b[32mFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 7, tokens None, triggered by: 0.24\n",
            "\u001b[34mcomputational ineï¬ ciency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 8, tokens None, triggered by: 0.15\n",
            "\u001b[35mFirst, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 9, tokens None, triggered by: 0.06\n",
            "\u001b[31mSecond, even though this change prevents the use of eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 10, tokens None, triggered by: 0.07\n",
            "\u001b[32mcient convolutions, we design a hardware-aware parallel algorithm in recurrent mode.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 11, tokens None, triggered by: 0.18\n",
            "\u001b[34mWe integrate these selective SSMs into a simpliï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 12, tokens None, triggered by: 0.16\n",
            "\u001b[35med end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5Ã\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 13, tokens None, triggered by: 0.08\n",
            "\u001b[31mhigher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 14, tokens None, triggered by: 0.16\n",
            "\u001b[32m# 1 Introduction\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 15, tokens None, triggered by: 0.22\n",
            "\u001b[34mFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 16, tokens None, triggered by: 0.28\n",
            "\u001b[35mective paradigm in modern machine learning.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 17, tokens None, triggered by: 0.18\n",
            "\u001b[31mThe backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The eï¬ cacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 18, tokens None, triggered by: 0.02\n",
            "\u001b[32mnite window, and quadratic scaling with respect to the window length.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 19, tokens None, triggered by: 0.23\n",
            "\u001b[34mAn enormous body of research has appeared on more eï¬ cient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 20, tokens None, triggered by: 0.17\n",
            "\u001b[35mective.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 21, tokens None, triggered by: 0.18\n",
            "\u001b[31mAs of yet, none of these variants have been shown to be empirically eï¬ ective at scale across domains.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 22, tokens None, triggered by: 0.21\n",
            "\u001b[32mRecently, structured state space sequence models (SSMs) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 23, tokens None, triggered by: 0.12\n",
            "\u001b[34mciently as either a recurrence or convolution, with linear or near-linear scaling in sequence length.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 24, tokens None, triggered by: 0.14\n",
            "\u001b[35mAdditionally, they have principled\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 25, tokens None, triggered by: 0.25\n",
            "\u001b[31mEqual contribution.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 26, tokens None, triggered by: -0.03\n",
            "\u001b[32m1\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 27, tokens None, triggered by: 0.04\n",
            "\u001b[34mmechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al. 2021).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 28, tokens None, triggered by: 0.13\n",
            "\u001b[35mMany ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 29, tokens None, triggered by: 0.24\n",
            "\u001b[31mavors of SSMs (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 30, tokens None, triggered by: 0.11\n",
            "\u001b[32mLi et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 31, tokens None, triggered by: 0.09\n",
            "\u001b[34mHowever, they have been less eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 32, tokens None, triggered by: 0.20\n",
            "\u001b[35mective at modeling discrete and information-dense data such as text.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 33, tokens None, triggered by: 0.13\n",
            "\u001b[31mWe propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 34, tokens None, triggered by: 0.25\n",
            "\u001b[32mSelection Mechanism. First, we identify a key limitation of prior models: the ability to eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 35, tokens None, triggered by: 0.19\n",
            "\u001b[34mciently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 36, tokens None, triggered by: 0.25\n",
            "\u001b[35mThis allows the model to ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 37, tokens None, triggered by: 0.10\n",
            "\u001b[31mlter out irrelevant information and remember relevant information indeï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 38, tokens None, triggered by: 0.04\n",
            "\u001b[32mnitely.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 39, tokens None, triggered by: 0.22\n",
            "\u001b[34mHardware-aware Algorithm.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 40, tokens None, triggered by: -0.02\n",
            "\u001b[35mThis simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 41, tokens None, triggered by: -0.01\n",
            "\u001b[31mcient.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 42, tokens None, triggered by: 0.25\n",
            "\u001b[32mWe overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between diï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 43, tokens None, triggered by: 0.13\n",
            "\u001b[34merent levels of the GPU memory hierarchy.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 44, tokens None, triggered by: 0.27\n",
            "\u001b[35mThe resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3Ã faster on A100 GPUs).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 45, tokens None, triggered by: 0.26\n",
            "\u001b[31mArchitecture.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 46, tokens None, triggered by: 0.18\n",
            "\u001b[32mWe simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and eï¬ ciency together yield performance improvements on real data up to sequence length 1M.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 47, tokens None, triggered by: 0.13\n",
            "\u001b[34mWe empirically validate Mambaâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 48, tokens None, triggered by: 0.18\n",
            "\u001b[35ms potential as a general sequence FM backbone, in both pretraining quality and domain-speciï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 49, tokens None, triggered by: 0.14\n",
            "\u001b[31mc task performance, on several types of modalities and settings:\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 50, tokens None, triggered by: 0.23\n",
            "\u001b[32mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 51, tokens None, triggered by: 0.26\n",
            "\u001b[34m¢ Synthetics.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 52, tokens None, triggered by: 0.15\n",
            "\u001b[35mOn important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indeï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 53, tokens None, triggered by: 0.14\n",
            "\u001b[31mnitely long (>1M tokens).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 54, tokens None, triggered by: 0.15\n",
            "\u001b[32mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 55, tokens None, triggered by: 0.27\n",
            "\u001b[34m¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform- ers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 56, tokens None, triggered by: 0.00\n",
            "\u001b[35mIn both settings, its performance improves with longer context up to million-length sequences.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 57, tokens None, triggered by: 0.21\n",
            "\u001b[31mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 58, tokens None, triggered by: 0.11\n",
            "\u001b[32m¢ Language Modeling.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 59, tokens None, triggered by: -0.05\n",
            "\u001b[34mMamba is the ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 60, tokens None, triggered by: -0.08\n",
            "\u001b[35mrst linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5Ã generation throughput compared to Transformers of similar size, and Mamba-3Bâ s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 61, tokens None, triggered by: 0.01\n",
            "\u001b[31mModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 62, tokens None, triggered by: 0.14\n",
            "\u001b[32m2\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 63, tokens None, triggered by: 0.03\n",
            "\u001b[34m# Selective State Space Model # with Hardware-aware State Expansion # A\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 64, tokens None, triggered by: 0.29\n",
            "\u001b[35mvuvy GPU SRAM Selection Mechanism es\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 65, tokens None, triggered by: 0.05\n",
            "\u001b[31mSelection Mechanism\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 66, tokens None, triggered by: 0.27\n",
            "\u001b[32mFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 67, tokens None, triggered by: 0.08\n",
            "\u001b[34m· = 5) of an input ð ¥ to output ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 68, tokens None, triggered by: 0.10\n",
            "\u001b[35m¦ through a higher dimensional latent state â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 69, tokens None, triggered by: 0.23\n",
            "\u001b[31m(e.g. ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 70, tokens None, triggered by: 0.11\n",
            "\u001b[32m= 4).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 71, tokens None, triggered by: 0.29\n",
            "\u001b[34mPrior SSMs avoid materializing this large effective state (ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 72, tokens None, triggered by: 0.17\n",
            "\u001b[35m·ð , times batch size ð µ and sequence length ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 73, tokens None, triggered by: 0.18\n",
            "\u001b[31m¿) through clever alternate computation paths requiring time-invariance: the (â , A, B, C) parameters are constant across time.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 74, tokens None, triggered by: 0.02\n",
            "\u001b[32mOur selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. # 2 State Space Models Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional function or sequence ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 75, tokens None, triggered by: 0.25\n",
            "\u001b[34m¥(ð ¡) â â â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 76, tokens None, triggered by: 0.05\n",
            "\u001b[35m¦ ð ¦(ð ¡) â â through an implicit latent state â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 77, tokens None, triggered by: 0.27\n",
            "\u001b[31m(ð ¡) â â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 78, tokens None, triggered by: 0.17\n",
            "\u001b[32mð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 79, tokens None, triggered by: 0.12\n",
            "\u001b[34m.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 80, tokens None, triggered by: 0.15\n",
            "\u001b[35mConcretely, S4 models are deï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 81, tokens None, triggered by: 0.27\n",
            "\u001b[31mned with four parameters (â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 82, tokens None, triggered by: 0.21\n",
            "\u001b[32m, A, B, C), which deï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 83, tokens None, triggered by: 0.12\n",
            "\u001b[34mne a sequence-to-sequence trans- formation in two stages.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 84, tokens None, triggered by: 0.21\n",
            "\u001b[35mâ â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 85, tokens None, triggered by: 0.21\n",
            "\u001b[31m²(ð ¡) = Aâ (ð ¡) + Bð ¥(ð ¡) ð ¦(ð ¡) = Câ (ð ¡)\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 86, tokens None, triggered by: 0.20\n",
            "\u001b[32m(1a) (1b) â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 87, tokens None, triggered by: 0.24\n",
            "\u001b[34mð ¡ = Aâ ð ¡â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 88, tokens None, triggered by: 0.19\n",
            "\u001b[35m1 + Bð ¥ð ¡ ð ¦ð ¡ = Câ ð ¡ (2a) (2b) ð ð ² = (Cð ©, Cð ¨ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 89, tokens None, triggered by: 0.26\n",
            "\u001b[31m©, â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 90, tokens None, triggered by: 0.22\n",
            "\u001b[32m¦ , Cð ¨ ð ¦ = ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 91, tokens None, triggered by: 0.26\n",
            "\u001b[34m¥ â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 92, tokens None, triggered by: 0.21\n",
            "\u001b[35mð ² ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 93, tokens None, triggered by: 0.20\n",
            "\u001b[31m©, â ¦ ) (3a) (3b)\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 94, tokens None, triggered by: 0.13\n",
            "\u001b[32mDiscretization.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 95, tokens None, triggered by: 0.14\n",
            "\u001b[34mThe ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 96, tokens None, triggered by: 0.04\n",
            "\u001b[35mrst stage transforms the â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 97, tokens None, triggered by: 0.17\n",
            "\u001b[31mcontinuous parametersâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 98, tokens None, triggered by: 0.24\n",
            "\u001b[32m(â , A, B) to â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 99, tokens None, triggered by: 0.30\n",
            "\u001b[34mdiscrete parametersâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 100, tokens None, triggered by: 0.29\n",
            "\u001b[35m(A, B) through ï¬ xed formulas A = ð ð ´(â , A) and B = ð ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 101, tokens None, triggered by: 0.12\n",
            "\u001b[31mµ(â , A, B), where the pair (ð ð ´, ð ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 102, tokens None, triggered by: 0.17\n",
            "\u001b[32mµ) is called a discretization rule. Various rules can be used such as the zero-order hold (ZOH) deï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 103, tokens None, triggered by: 0.00\n",
            "\u001b[34mned in equation (4). A = exp(â A) B = (â A)â 1(exp(â A) â I) â â B (4)\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 104, tokens None, triggered by: 0.19\n",
            "\u001b[35mDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized (Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 105, tokens None, triggered by: 0.13\n",
            "\u001b[31mIt also has connections to gating mechanisms of RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 106, tokens None, triggered by: 0.10\n",
            "\u001b[32mHowever, from a mechanical point of view discretization can simply be viewed as the ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 107, tokens None, triggered by: 0.02\n",
            "\u001b[34mrst step of the computation graph in the forward pass of an SSM.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 108, tokens None, triggered by: 0.03\n",
            "\u001b[35mAlternate ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 109, tokens None, triggered by: 0.14\n",
            "\u001b[31mavors of SSMs can bypass the discretization step and parameterize (A, B) directly instead (Zhang et al. 2023), which may be easier to reason about.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 110, tokens None, triggered by: 0.27\n",
            "\u001b[32mComputation.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 111, tokens None, triggered by: 0.27\n",
            "\u001b[34mAfter the parameters have been transformed from (â , A, B, C) â ¦ (A, B, C), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 112, tokens None, triggered by: 0.12\n",
            "\u001b[35m3\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 113, tokens None, triggered by: 0.26\n",
            "\u001b[31mCommonly, the model uses the convolutional mode (3) for eï¬ cient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for eï¬ cient autoregressive inference (where the inputs are seen one timestep at a time). Linear Time Invariance (LTI).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 114, tokens None, triggered by: 0.17\n",
            "\u001b[32mAn important property of equations (1) to (3) is that the modelâ s dynamics are constant through time.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 115, tokens None, triggered by: 0.06\n",
            "\u001b[34mIn other words (â , A, B, C), and consequently (A, B) as well, are ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 116, tokens None, triggered by: 0.08\n",
            "\u001b[35mxed for all time-steps.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 117, tokens None, triggered by: 0.19\n",
            "\u001b[31mThis property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions. Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models. Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 118, tokens None, triggered by: 0.26\n",
            "\u001b[32mciency constraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 119, tokens None, triggered by: 0.12\n",
            "\u001b[34mciency bottlenecks.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 120, tokens None, triggered by: 0.23\n",
            "\u001b[35mStructure and Dimensions.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 121, tokens None, triggered by: 0.17\n",
            "\u001b[31mFinally, we note that structured SSMs are so named because computing them eï¬ ciently also requires imposing structure on the A matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 122, tokens None, triggered by: 0.27\n",
            "\u001b[32mIn this case, the A â â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 123, tokens None, triggered by: 0.27\n",
            "\u001b[34mð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 124, tokens None, triggered by: 0.27\n",
            "\u001b[35mÃ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 125, tokens None, triggered by: 0.16\n",
            "\u001b[31mð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 126, tokens None, triggered by: 0.27\n",
            "\u001b[32m, B â â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 127, tokens None, triggered by: 0.27\n",
            "\u001b[34mð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 128, tokens None, triggered by: 0.26\n",
            "\u001b[35mÃ 1, C â â 1Ã\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 129, tokens None, triggered by: 0.20\n",
            "\u001b[31mð matrices can all be represented by ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 130, tokens None, triggered by: 0.02\n",
            "\u001b[32mnumbers. To operate over an input sequence ð ¥ of batch size ð µ and length ð ¿ with ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 131, tokens None, triggered by: 0.20\n",
            "\u001b[34m· channels, the SSM is applied independently to each channel.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 132, tokens None, triggered by: 0.22\n",
            "\u001b[35mNote that in this case, the total hidden state has dimension ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 133, tokens None, triggered by: 0.24\n",
            "\u001b[31m·ð per input, and computing it over the sequence length requires ð (ð µð ¿ð ·ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 134, tokens None, triggered by: 0.18\n",
            "\u001b[32m) time and memory; this is the root of the fundamental eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 135, tokens None, triggered by: 0.12\n",
            "\u001b[34mciency bottleneck addressed in Section 3.3.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 136, tokens None, triggered by: 0.18\n",
            "\u001b[35mGeneral State Space Models. We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 137, tokens None, triggered by: 0.14\n",
            "\u001b[31mIt has been used to refer to many disparate concepts in diï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 138, tokens None, triggered by: 0.20\n",
            "\u001b[32merent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman ï¬ lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 139, tokens None, triggered by: 0.07\n",
            "\u001b[34mThroughout this entire paper we use the term â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 140, tokens None, triggered by: 0.29\n",
            "\u001b[35mSSMâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 141, tokens None, triggered by: 0.21\n",
            "\u001b[31mto refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 142, tokens None, triggered by: 0.16\n",
            "\u001b[32mFor convenience we may also include derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 143, tokens None, triggered by: 0.06\n",
            "\u001b[34mLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 144, tokens None, triggered by: 0.09\n",
            "\u001b[35mSSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 145, tokens None, triggered by: 0.17\n",
            "\u001b[31mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 146, tokens None, triggered by: 0.17\n",
            "\u001b[32m¢ Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 147, tokens None, triggered by: 0.12\n",
            "\u001b[34mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 148, tokens None, triggered by: 0.03\n",
            "\u001b[35m¢ H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 149, tokens None, triggered by: 0.08\n",
            "\u001b[31mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 150, tokens None, triggered by: 0.08\n",
            "\u001b[32m¢ Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 151, tokens None, triggered by: 0.19\n",
            "\u001b[34mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 152, tokens None, triggered by: 0.17\n",
            "\u001b[35m¢ RetNet (Y.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 153, tokens None, triggered by: 0.08\n",
            "\u001b[31mSun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 154, tokens None, triggered by: 0.08\n",
            "\u001b[32m4 â ¢ RWKV (B.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 155, tokens None, triggered by: 0.02\n",
            "\u001b[34mPeng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation (attention-free Transformer (S.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 156, tokens None, triggered by: 0.08\n",
            "\u001b[35mZhai et al. 2021)).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 157, tokens None, triggered by: 0.14\n",
            "\u001b[31mIts main â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 158, tokens None, triggered by: -0.03\n",
            "\u001b[32mWKVâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 159, tokens None, triggered by: 0.13\n",
            "\u001b[34mmechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. # 3 Selective State Space Models We motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 160, tokens None, triggered by: 0.05\n",
            "\u001b[35mThe resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 161, tokens None, triggered by: 0.02\n",
            "\u001b[31mciently.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 162, tokens None, triggered by: 0.28\n",
            "\u001b[32mWe overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 163, tokens None, triggered by: 0.21\n",
            "\u001b[34mFinally, we discuss some additional properties of selection mechanisms (Section 3.5). # 3.1 Motivation:\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 164, tokens None, triggered by: 0.15\n",
            "\u001b[35mSelection as a Means of Compression We argue that a fundamental problem of sequence modeling is compressing context into a smaller state.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 165, tokens None, triggered by: 0.22\n",
            "\u001b[31mIn fact, we can view the tradeoï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 166, tokens None, triggered by: 0.15\n",
            "\u001b[32ms of popular sequence models from this point of view.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 167, tokens None, triggered by: 0.23\n",
            "\u001b[34mFor example, attention is both eï¬ ective and ineï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 168, tokens None, triggered by: 0.22\n",
            "\u001b[35mcient because it explicitly does not compress context at all.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 169, tokens None, triggered by: 0.29\n",
            "\u001b[31mThis can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 170, tokens None, triggered by: 0.25\n",
            "\u001b[32mOn the other hand, recurrent models are eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 171, tokens None, triggered by: 0.09\n",
            "\u001b[34mcient because they have a ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 172, tokens None, triggered by: 0.05\n",
            "\u001b[35mnite state, implying constant-time inference and linear-time training.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 173, tokens None, triggered by: 0.19\n",
            "\u001b[31mHowever, their eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 174, tokens None, triggered by: 0.28\n",
            "\u001b[32mectiveness is limited by how well this state has compressed the context.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 175, tokens None, triggered by: 0.10\n",
            "\u001b[34mTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 176, tokens None, triggered by: 0.16\n",
            "\u001b[35mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 177, tokens None, triggered by: 0.24\n",
            "\u001b[31m¢ The Selective Copying task modiï¬ es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 178, tokens None, triggered by: 0.21\n",
            "\u001b[32mlter out the irrelevant ones (white).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 179, tokens None, triggered by: 0.05\n",
            "\u001b[34mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 180, tokens None, triggered by: 0.23\n",
            "\u001b[35m¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 181, tokens None, triggered by: 0.18\n",
            "\u001b[31mThese tasks reveal the failure mode of LTI models.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 182, tokens None, triggered by: 0.17\n",
            "\u001b[32mFrom the recurrent view, their constant dynamics (e.g. the (A, B) transitions in (2)) cannot let them select the correct information from their context, or aï¬ ect the hidden state passed along the sequence an in input-dependent way.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 183, tokens None, triggered by: 0.08\n",
            "\u001b[34mFrom the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have diï¬ culty with the Selective Copying task because of lack of content-awareness (Figure 2).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 184, tokens None, triggered by: 0.01\n",
            "\u001b[35mMore concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 185, tokens None, triggered by: 0.16\n",
            "\u001b[31mIn summary, the eï¬ ciency vs. eï¬ ectiveness tradeoï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 186, tokens None, triggered by: 0.29\n",
            "\u001b[32mof sequence models is characterized by how well they compress their state: eï¬ cient models must have a small state, while eï¬ ective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 187, tokens None, triggered by: 0.26\n",
            "\u001b[34mlter out inputs into a sequential state.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 188, tokens None, triggered by: 0.23\n",
            "\u001b[35mIn particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 189, tokens None, triggered by: 0.20\n",
            "\u001b[31m# Improving SSMs with Selection One method of incorporating a selection mechanism into models is by letting their parameters that aï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 190, tokens None, triggered by: final split\n",
            "\u001b[32mect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the c\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "* Depending upon the similarity threshold you set, the chunks can be too small or too big.\n",
        "* It appears the chunks above are too small so we may want to up the threshold."
      ],
      "metadata": {
        "id": "01aeFx2wK_to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Cumulative Chunker\n",
        "* Cumulatively adding chunks of text --> creating embeddings --> testing cosine similarity --> creates chunk based on cumulative embeddings.\n",
        "* Creates more embeddings.\n",
        "* More expensive if using API.\n",
        "* More compute power.\n",
        "* MORE NOISE resistant.\n",
        "* Results can be worse than statistical chunker."
      ],
      "metadata": {
        "id": "aWpVS4G6LgKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## import\n",
        "from semantic_chunkers import CumulativeChunker\n",
        "\n",
        "## setup chunker\n",
        "cum_chunker = CumulativeChunker(encoder=encoder, score_threshold=0.3) ## change threshold"
      ],
      "metadata": {
        "id": "ttCspZOtKxDD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## chunks\n",
        "cum_chunks = cum_chunker(docs=[content])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e8519f05f1ea43aba5e41bd9d4e18fab",
            "fea86f1dad7b40fb94daf6219404a877",
            "9d1525821d864c829c91715d1dd79d29",
            "900444a5dad2429c9b934393579adeab",
            "eb6298029a5f4159b26fe79905a67497",
            "42effea6bfbe47e8b3aea1b0a801ceb7",
            "3827cd61c13241b48e00a76204a9c6ba",
            "abaf97de0ca140988ee62a5a8a8b5dbc",
            "e71a8f2139dc40a281e29605eb4885b9",
            "129d612659394f019211ed3c95cecec0",
            "75b327dccc4d44d1aef5482402799479"
          ]
        },
        "id": "3MzRWovJMS7f",
        "outputId": "db2b40de-951a-40c9-f44c-4b51909fcb20"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/329 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8519f05f1ea43aba5e41bd9d4e18fab"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## print cumulative chunks\n",
        "cum_chunker.print(cum_chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkYrxFPqMYcI",
        "outputId": "2e9b0bee-d8b2-4043-b1ad-3cbdb23eeb41"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1, tokens None, triggered by: 0.06\n",
            "\u001b[31m# Mamba:\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 2, tokens None, triggered by: 0.08\n",
            "\u001b[32mLinear-Time Sequence Modeling with Selective State Spaces\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 3, tokens None, triggered by: 0.08\n",
            "\u001b[34m# Albert Gu*1 and Tri Dao*2\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 4, tokens None, triggered by: 0.05\n",
            "\u001b[35m1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Princeton University agu@cs.cmu.edu, tri@tridao.me\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 5, tokens None, triggered by: 0.15\n",
            "\u001b[31m# Abstract\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 6, tokens None, triggered by: 0.09\n",
            "\u001b[32mFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 7, tokens None, triggered by: 0.29\n",
            "\u001b[34mcomputational ineï¬ ciency on long sequences, but they have not performed as well as attention on important modalities such as language.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 8, tokens None, triggered by: 0.24\n",
            "\u001b[35mWe identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 9, tokens None, triggered by: 0.15\n",
            "\u001b[31mFirst, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 10, tokens None, triggered by: 0.06\n",
            "\u001b[32mSecond, even though this change prevents the use of eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 11, tokens None, triggered by: 0.07\n",
            "\u001b[34mcient convolutions, we design a hardware-aware parallel algorithm in recurrent mode.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 12, tokens None, triggered by: 0.18\n",
            "\u001b[35mWe integrate these selective SSMs into a simpliï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 13, tokens None, triggered by: 0.24\n",
            "\u001b[31med end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5Ã\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 14, tokens None, triggered by: 0.06\n",
            "\u001b[32mhigher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 15, tokens None, triggered by: 0.16\n",
            "\u001b[34m# 1 Introduction\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 16, tokens None, triggered by: 0.22\n",
            "\u001b[35mFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 17, tokens None, triggered by: 0.28\n",
            "\u001b[31mective paradigm in modern machine learning.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 18, tokens None, triggered by: 0.29\n",
            "\u001b[32mThe backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The eï¬ cacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 19, tokens None, triggered by: 0.18\n",
            "\u001b[34mHowever, this property brings fundamental drawbacks: an inability to model anything outside of a ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 20, tokens None, triggered by: 0.02\n",
            "\u001b[35mnite window, and quadratic scaling with respect to the window length.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 21, tokens None, triggered by: 0.18\n",
            "\u001b[31mAn enormous body of research has appeared on more eï¬ cient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 22, tokens None, triggered by: 0.17\n",
            "\u001b[32mective.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 23, tokens None, triggered by: 0.09\n",
            "\u001b[34mAs of yet, none of these variants have been shown to be empirically eï¬ ective at scale across domains.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 24, tokens None, triggered by: 0.24\n",
            "\u001b[35mRecently, structured state space sequence models (SSMs) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models (Kalman 1960).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 25, tokens None, triggered by: 0.21\n",
            "\u001b[31mThis class of models can be computed very eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 26, tokens None, triggered by: 0.12\n",
            "\u001b[32mciently as either a recurrence or convolution, with linear or near-linear scaling in sequence length.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 27, tokens None, triggered by: 0.14\n",
            "\u001b[34mAdditionally, they have principled\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 28, tokens None, triggered by: 0.25\n",
            "\u001b[35mEqual contribution.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 29, tokens None, triggered by: -0.03\n",
            "\u001b[31m1\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 30, tokens None, triggered by: 0.04\n",
            "\u001b[32mmechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al. 2021).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 31, tokens None, triggered by: 0.13\n",
            "\u001b[34mMany ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 32, tokens None, triggered by: 0.24\n",
            "\u001b[35mavors of SSMs (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 33, tokens None, triggered by: 0.11\n",
            "\u001b[31mLi et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 34, tokens None, triggered by: 0.09\n",
            "\u001b[32mHowever, they have been less eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 35, tokens None, triggered by: 0.20\n",
            "\u001b[34mective at modeling discrete and information-dense data such as text.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 36, tokens None, triggered by: 0.13\n",
            "\u001b[35mWe propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 37, tokens None, triggered by: 0.26\n",
            "\u001b[31mSelection Mechanism. First, we identify a key limitation of prior models: the ability to eï¬ ciently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 38, tokens None, triggered by: 0.25\n",
            "\u001b[32mThis allows the model to ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 39, tokens None, triggered by: 0.10\n",
            "\u001b[34mlter out irrelevant information and remember relevant information indeï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 40, tokens None, triggered by: 0.04\n",
            "\u001b[35mnitely.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 41, tokens None, triggered by: 0.22\n",
            "\u001b[31mHardware-aware Algorithm.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 42, tokens None, triggered by: -0.02\n",
            "\u001b[32mThis simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 43, tokens None, triggered by: -0.01\n",
            "\u001b[34mcient.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 44, tokens None, triggered by: 0.25\n",
            "\u001b[35mWe overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between diï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 45, tokens None, triggered by: 0.13\n",
            "\u001b[31merent levels of the GPU memory hierarchy.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 46, tokens None, triggered by: 0.09\n",
            "\u001b[32mThe resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3Ã faster on A100 GPUs).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 47, tokens None, triggered by: 0.26\n",
            "\u001b[34mArchitecture.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 48, tokens None, triggered by: 0.19\n",
            "\u001b[35mWe simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and eï¬ ciency together yield performance improvements on real data up to sequence length 1M.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 49, tokens None, triggered by: 0.13\n",
            "\u001b[31mWe empirically validate Mambaâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 50, tokens None, triggered by: 0.18\n",
            "\u001b[32ms potential as a general sequence FM backbone, in both pretraining quality and domain-speciï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 51, tokens None, triggered by: 0.14\n",
            "\u001b[34mc task performance, on several types of modalities and settings:\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 52, tokens None, triggered by: 0.23\n",
            "\u001b[35mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 53, tokens None, triggered by: 0.26\n",
            "\u001b[31m¢ Synthetics.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 54, tokens None, triggered by: 0.15\n",
            "\u001b[32mOn important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indeï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 55, tokens None, triggered by: 0.14\n",
            "\u001b[34mnitely long (>1M tokens).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 56, tokens None, triggered by: 0.15\n",
            "\u001b[35mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 57, tokens None, triggered by: 0.28\n",
            "\u001b[31m¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform- ers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 58, tokens None, triggered by: 0.00\n",
            "\u001b[32mIn both settings, its performance improves with longer context up to million-length sequences.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 59, tokens None, triggered by: 0.21\n",
            "\u001b[34mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 60, tokens None, triggered by: 0.11\n",
            "\u001b[35m¢ Language Modeling.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 61, tokens None, triggered by: -0.05\n",
            "\u001b[31mMamba is the ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 62, tokens None, triggered by: 0.30\n",
            "\u001b[32mrst linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 63, tokens None, triggered by: 0.26\n",
            "\u001b[34mOur Mamba language model has 5Ã generation throughput compared to Transformers of similar size, and Mamba-3Bâ s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 64, tokens None, triggered by: 0.01\n",
            "\u001b[35mModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 65, tokens None, triggered by: 0.14\n",
            "\u001b[31m2\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 66, tokens None, triggered by: 0.27\n",
            "\u001b[32m# Selective State Space Model # with Hardware-aware State Expansion\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 67, tokens None, triggered by: 0.03\n",
            "\u001b[34m# A\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 68, tokens None, triggered by: 0.29\n",
            "\u001b[35mvuvy GPU SRAM Selection Mechanism es\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 69, tokens None, triggered by: 0.05\n",
            "\u001b[31mSelection Mechanism\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 70, tokens None, triggered by: 0.27\n",
            "\u001b[32mFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 71, tokens None, triggered by: 0.14\n",
            "\u001b[34m· = 5) of an input ð ¥ to output ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 72, tokens None, triggered by: 0.10\n",
            "\u001b[35m¦ through a higher dimensional latent state â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 73, tokens None, triggered by: 0.23\n",
            "\u001b[31m(e.g. ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 74, tokens None, triggered by: 0.11\n",
            "\u001b[32m= 4).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 75, tokens None, triggered by: 0.29\n",
            "\u001b[34mPrior SSMs avoid materializing this large effective state (ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 76, tokens None, triggered by: 0.24\n",
            "\u001b[35m·ð , times batch size ð µ and sequence length ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 77, tokens None, triggered by: 0.26\n",
            "\u001b[31m¿) through clever alternate computation paths requiring time-invariance: the (â , A, B, C) parameters are constant across time.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 78, tokens None, triggered by: -0.03\n",
            "\u001b[32mOur selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. # 2 State Space Models Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional function or sequence ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 79, tokens None, triggered by: 0.24\n",
            "\u001b[34m¥(ð ¡) â â â ¦ ð ¦(ð ¡) â â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 80, tokens None, triggered by: 0.05\n",
            "\u001b[35mthrough an implicit latent state â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 81, tokens None, triggered by: 0.08\n",
            "\u001b[31m(ð ¡) â â ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 82, tokens None, triggered by: 0.12\n",
            "\u001b[32m.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 83, tokens None, triggered by: 0.15\n",
            "\u001b[34mConcretely, S4 models are deï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 84, tokens None, triggered by: 0.27\n",
            "\u001b[35mned with four parameters (â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 85, tokens None, triggered by: 0.21\n",
            "\u001b[31m, A, B, C), which deï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 86, tokens None, triggered by: 0.12\n",
            "\u001b[32mne a sequence-to-sequence trans- formation in two stages.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 87, tokens None, triggered by: 0.17\n",
            "\u001b[34mâ â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 88, tokens None, triggered by: 0.26\n",
            "\u001b[35m²(ð ¡) = Aâ (ð ¡) + Bð ¥(ð ¡) ð ¦(ð ¡) = Câ (ð ¡)\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 89, tokens None, triggered by: 0.20\n",
            "\u001b[31m(1a) (1b) â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 90, tokens None, triggered by: 0.27\n",
            "\u001b[32mð ¡ = Aâ ð ¡â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 91, tokens None, triggered by: 0.08\n",
            "\u001b[34m1 + Bð ¥ð ¡ ð ¦ð ¡ = Câ ð ¡ (2a) (2b) ð ð ² = (Cð ©, Cð ¨ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 92, tokens None, triggered by: 0.26\n",
            "\u001b[35m©, â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 93, tokens None, triggered by: 0.11\n",
            "\u001b[31m¦ , Cð ¨ ð ¦ = ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 94, tokens None, triggered by: 0.26\n",
            "\u001b[32m¥ â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 95, tokens None, triggered by: 0.14\n",
            "\u001b[34mð ² ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 96, tokens None, triggered by: 0.21\n",
            "\u001b[35m©, â ¦ ) (3a) (3b)\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 97, tokens None, triggered by: 0.13\n",
            "\u001b[31mDiscretization.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 98, tokens None, triggered by: 0.14\n",
            "\u001b[32mThe ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 99, tokens None, triggered by: 0.04\n",
            "\u001b[34mrst stage transforms the â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 100, tokens None, triggered by: 0.17\n",
            "\u001b[35mcontinuous parametersâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 101, tokens None, triggered by: 0.25\n",
            "\u001b[31m(â , A, B) to â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 102, tokens None, triggered by: 0.30\n",
            "\u001b[32mdiscrete parametersâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 103, tokens None, triggered by: 0.29\n",
            "\u001b[34m(A, B) through ï¬ xed formulas A = ð ð ´(â , A) and B = ð ð µ(â , A, B), where the pair (ð ð ´, ð ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 104, tokens None, triggered by: 0.17\n",
            "\u001b[35mµ) is called a discretization rule. Various rules can be used such as the zero-order hold (ZOH) deï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 105, tokens None, triggered by: 0.30\n",
            "\u001b[31mned in equation (4). A = exp(â A) B = (â A)â 1(exp(â A) â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 106, tokens None, triggered by: -0.01\n",
            "\u001b[32mI) â â B (4)\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 107, tokens None, triggered by: 0.19\n",
            "\u001b[34mDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized (Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 108, tokens None, triggered by: 0.13\n",
            "\u001b[35mIt also has connections to gating mechanisms of RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 109, tokens None, triggered by: 0.10\n",
            "\u001b[31mHowever, from a mechanical point of view discretization can simply be viewed as the ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 110, tokens None, triggered by: 0.02\n",
            "\u001b[32mrst step of the computation graph in the forward pass of an SSM.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 111, tokens None, triggered by: 0.03\n",
            "\u001b[34mAlternate ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 112, tokens None, triggered by: 0.14\n",
            "\u001b[35mavors of SSMs can bypass the discretization step and parameterize (A, B) directly instead (Zhang et al. 2023), which may be easier to reason about.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 113, tokens None, triggered by: 0.27\n",
            "\u001b[31mComputation.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 114, tokens None, triggered by: 0.22\n",
            "\u001b[32mAfter the parameters have been transformed from (â , A, B, C) â ¦ (A, B, C), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 115, tokens None, triggered by: 0.12\n",
            "\u001b[34m3\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 116, tokens None, triggered by: 0.28\n",
            "\u001b[35mCommonly, the model uses the convolutional mode (3) for eï¬ cient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for eï¬ cient autoregressive inference (where the inputs are seen one timestep at a time).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 117, tokens None, triggered by: 0.26\n",
            "\u001b[31mLinear Time Invariance (LTI).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 118, tokens None, triggered by: 0.17\n",
            "\u001b[32mAn important property of equations (1) to (3) is that the modelâ s dynamics are constant through time.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 119, tokens None, triggered by: 0.07\n",
            "\u001b[34mIn other words (â , A, B, C), and consequently (A, B) as well, are ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 120, tokens None, triggered by: 0.08\n",
            "\u001b[35mxed for all time-steps.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 121, tokens None, triggered by: 0.13\n",
            "\u001b[31mThis property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions. Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models. Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 122, tokens None, triggered by: 0.10\n",
            "\u001b[32mciency constraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the eï¬ ciency bottlenecks.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 123, tokens None, triggered by: 0.23\n",
            "\u001b[34mStructure and Dimensions.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 124, tokens None, triggered by: 0.14\n",
            "\u001b[35mFinally, we note that structured SSMs are so named because computing them eï¬ ciently also requires imposing structure on the A matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 125, tokens None, triggered by: 0.21\n",
            "\u001b[31mIn this case, the A â â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 126, tokens None, triggered by: 0.27\n",
            "\u001b[32mð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 127, tokens None, triggered by: 0.27\n",
            "\u001b[34mÃ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 128, tokens None, triggered by: 0.16\n",
            "\u001b[35mð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 129, tokens None, triggered by: 0.16\n",
            "\u001b[31m, B â â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 130, tokens None, triggered by: 0.27\n",
            "\u001b[32mð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 131, tokens None, triggered by: 0.12\n",
            "\u001b[34mÃ 1, C â â 1Ã\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 132, tokens None, triggered by: 0.19\n",
            "\u001b[35mð matrices can all be represented by ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 133, tokens None, triggered by: 0.18\n",
            "\u001b[31mnumbers. To operate over an input sequence ð ¥ of batch size ð µ and length ð ¿ with ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 134, tokens None, triggered by: 0.20\n",
            "\u001b[32m· channels, the SSM is applied independently to each channel.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 135, tokens None, triggered by: 0.22\n",
            "\u001b[34mNote that in this case, the total hidden state has dimension ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 136, tokens None, triggered by: 0.28\n",
            "\u001b[35m·ð per input, and computing it over the sequence length requires ð (ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 137, tokens None, triggered by: 0.22\n",
            "\u001b[31mµð ¿ð ·ð\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 138, tokens None, triggered by: 0.18\n",
            "\u001b[32m) time and memory; this is the root of the fundamental eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 139, tokens None, triggered by: 0.12\n",
            "\u001b[34mciency bottleneck addressed in Section 3.3.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 140, tokens None, triggered by: 0.12\n",
            "\u001b[35mGeneral State Space Models. We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 141, tokens None, triggered by: 0.14\n",
            "\u001b[31mIt has been used to refer to many disparate concepts in diï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 142, tokens None, triggered by: 0.20\n",
            "\u001b[32merent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman ï¬ lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 143, tokens None, triggered by: 0.07\n",
            "\u001b[34mThroughout this entire paper we use the term â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 144, tokens None, triggered by: 0.29\n",
            "\u001b[35mSSMâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 145, tokens None, triggered by: 0.21\n",
            "\u001b[31mto refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 146, tokens None, triggered by: 0.16\n",
            "\u001b[32mFor convenience we may also include derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 147, tokens None, triggered by: 0.06\n",
            "\u001b[34mLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 148, tokens None, triggered by: 0.11\n",
            "\u001b[35mSSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 149, tokens None, triggered by: 0.17\n",
            "\u001b[31mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 150, tokens None, triggered by: 0.17\n",
            "\u001b[32m¢ Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 151, tokens None, triggered by: 0.12\n",
            "\u001b[34mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 152, tokens None, triggered by: 0.11\n",
            "\u001b[35m¢ H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 153, tokens None, triggered by: 0.08\n",
            "\u001b[31mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 154, tokens None, triggered by: 0.08\n",
            "\u001b[32m¢ Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 155, tokens None, triggered by: 0.19\n",
            "\u001b[34mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 156, tokens None, triggered by: 0.17\n",
            "\u001b[35m¢ RetNet (Y.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 157, tokens None, triggered by: 0.08\n",
            "\u001b[31mSun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 158, tokens None, triggered by: 0.07\n",
            "\u001b[32m4 â ¢ RWKV (B.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 159, tokens None, triggered by: 0.02\n",
            "\u001b[34mPeng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation (attention-free Transformer (S.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 160, tokens None, triggered by: 0.08\n",
            "\u001b[35mZhai et al. 2021)).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 161, tokens None, triggered by: 0.14\n",
            "\u001b[31mIts main â\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 162, tokens None, triggered by: -0.03\n",
            "\u001b[32mWKVâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 163, tokens None, triggered by: 0.28\n",
            "\u001b[34mmechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 164, tokens None, triggered by: 0.15\n",
            "\u001b[35m# 3 Selective State Space Models We motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 165, tokens None, triggered by: 0.05\n",
            "\u001b[31mThe resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 166, tokens None, triggered by: 0.02\n",
            "\u001b[32mciently.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 167, tokens None, triggered by: 0.16\n",
            "\u001b[34mWe overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 168, tokens None, triggered by: 0.12\n",
            "\u001b[35mFinally, we discuss some additional properties of selection mechanisms (Section 3.5). # 3.1 Motivation: Selection as a Means of Compression We argue that a fundamental problem of sequence modeling is compressing context into a smaller state.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 169, tokens None, triggered by: 0.22\n",
            "\u001b[31mIn fact, we can view the tradeoï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 170, tokens None, triggered by: 0.15\n",
            "\u001b[32ms of popular sequence models from this point of view.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 171, tokens None, triggered by: 0.22\n",
            "\u001b[34mFor example, attention is both eï¬ ective and ineï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 172, tokens None, triggered by: 0.22\n",
            "\u001b[35mcient because it explicitly does not compress context at all.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 173, tokens None, triggered by: 0.29\n",
            "\u001b[31mThis can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 174, tokens None, triggered by: 0.25\n",
            "\u001b[32mOn the other hand, recurrent models are eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 175, tokens None, triggered by: 0.09\n",
            "\u001b[34mcient because they have a ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 176, tokens None, triggered by: 0.05\n",
            "\u001b[35mnite state, implying constant-time inference and linear-time training.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 177, tokens None, triggered by: 0.19\n",
            "\u001b[31mHowever, their eï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 178, tokens None, triggered by: 0.28\n",
            "\u001b[32mectiveness is limited by how well this state has compressed the context.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 179, tokens None, triggered by: 0.10\n",
            "\u001b[34mTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 180, tokens None, triggered by: 0.16\n",
            "\u001b[35mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 181, tokens None, triggered by: 0.13\n",
            "\u001b[31m¢ The Selective Copying task modiï¬ es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and ï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 182, tokens None, triggered by: 0.21\n",
            "\u001b[32mlter out the irrelevant ones (white).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 183, tokens None, triggered by: 0.05\n",
            "\u001b[34mâ\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 184, tokens None, triggered by: 0.16\n",
            "\u001b[35m¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 185, tokens None, triggered by: 0.18\n",
            "\u001b[31mThese tasks reveal the failure mode of LTI models.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 186, tokens None, triggered by: 0.23\n",
            "\u001b[32mFrom the recurrent view, their constant dynamics (e.g. the (A, B) transitions in (2)) cannot let them select the correct information from their context, or aï¬ ect the hidden state passed along the sequence an in input-dependent way.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 187, tokens None, triggered by: -0.00\n",
            "\u001b[34mFrom the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have diï¬ culty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels.\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 188, tokens None, triggered by: 0.16\n",
            "\u001b[35mIn summary, the eï¬ ciency vs. eï¬ ectiveness tradeoï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 189, tokens None, triggered by: 0.25\n",
            "\u001b[31mof sequence models is characterized by how well they compress their state: eï¬ cient models must have a small state, while eï¬ ective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or ï¬ lter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion).\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 190, tokens None, triggered by: 0.14\n",
            "\u001b[32m# Improving SSMs with Selection One method of incorporating a selection mechanism into models is by letting their parameters that aï¬\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Split 191, tokens None, triggered by: final split\n",
            "\u001b[34mect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the c\u001b[0m\n",
            "----------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Semantic Text Splitter\n",
        "* Future semantic chunker to try is the langchain chunker.\n",
        "* Documentation here: https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/"
      ],
      "metadata": {
        "id": "48km85qrNY10"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "82nfK4cuNsCy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}